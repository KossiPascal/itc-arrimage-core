from time import sleep
from typing import Any
from psycopg2 import sql, OperationalError, DatabaseError, InterfaceError
from psycopg2.extras import Json, execute_values, RealDictCursor
from utils.config import config
from datetime import datetime, date, timezone, time
from utils.db import get_connection
from utils.functions import to_datetime
from utils.hasher_uitls import hash_password

from utils.logger import get_logger
logger = get_logger(__name__)

DHIS2_TABLE_KEY = {
    "events": "id",
    "attributes": "id",
    "enrollments": "id",
    "dataElements": "id",
    "organisationUnits": "id",
    "trackedEntityInstances": "id",
}


class PostgresClient:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        self.force_init = config.FORCE_INIT_CLASS

        if self._initialized and self.force_init is not True:
            return

        self.conn = get_connection()
        if not self.conn:
            raise ValueError("‚ùå Erreur de connexion √† PostgreSQL : connexion nulle")

        # Caches pour √©viter de refaire les v√©rifications
        self._verified_tables = set()
        self._verified_columns = {}  # table_name -> set(columns)
        self._verified_pk = set()    # (table_name, id_field)

        self.ensure_tables()
        self.create_default_admin()

        self._initialized = True


    def normalize_tablename(self, tablename: str) -> str:
        """
        Convert table or column names to lowercase
        and remove double quotes to avoid case-sensitive identifiers.
        """

        # # --- Validation s√©curis√©e ---
        # if not isinstance(tablename, str):
        #     raise ValueError("table must be a string")
        
        # if not tablename:
        #     return tablename
        # tablename = tablename.replace('"', '')
        # return tablename.lower()
        return tablename

    def convert_value_for_pg(self, value):
        """Convertit automatiquement toutes les valeurs en formats compatibles PostgreSQL."""
        # JSONB
        if isinstance(value, (dict, list, set, tuple)):
            return Json(value)
        # Dates, datetimes ‚Üí string ISO (psycopg2 g√®re ensuite)
        if isinstance(value, (datetime, date)):
            return value.isoformat()
        return value

    def guess_pg_type(self, value, colname=None, id_field=None):
        """
        D√©tecte automatiquement le type SQL PostgreSQL pour une valeur donn√©e.
        - id_field DHIS2 ‚Üí TEXT
        - bool ‚Üí BOOLEAN
        - int ‚Üí BIGINT
        - float ‚Üí DOUBLE PRECISION
        - list/dict ‚Üí JSONB
        - datetime/date/time ‚Üí TIMESTAMP WITH TIME ZONE ou DATE
        - Autres ‚Üí TEXT
        """

        # ID DHIS2 ou valeur None ‚Üí TEXT
        if colname is not None and colname == id_field or value is None:
            return "TEXT"
        if isinstance(value, bool): return "BOOLEAN"
        if isinstance(value, int): return "BIGINT"
        if isinstance(value, float): return "DOUBLE PRECISION"
        if isinstance(value, (list, dict, set, tuple)): return "JSONB"
        if isinstance(value, datetime): return "TIMESTAMP WITH TIME ZONE"
        if isinstance(value, date): return "DATE"
        if isinstance(value, time): return "TIME"
        # Tout le reste (str, etc.)
        return "TEXT"
    
    def ensure_tables(self) -> bool:
        if "base_tables" in self._verified_tables and self.force_init is not True:
            return
        
        try:
            with self.conn.cursor() as cur:
                # Data_elements
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS "dataElements" (
                        id TEXT PRIMARY KEY,
                        name TEXT,
                        shortname TEXT,
                        synced_at TIMESTAMP DEFAULT now()
                    );
                """)

                # Users
                # role TEXT NOT NULL DEFAULT 'user' CHECK (role IN ('user', 'admin', 'superadmin')),
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS users (
                        id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                        fullname TEXT,
                        username TEXT UNIQUE NOT NULL,
                        password TEXT NOT NULL,
                        role TEXT NOT NULL DEFAULT 'user',
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    );
                """)

                # Refresh tokens
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS refresh_tokens (
                        id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                        username TEXT NOT NULL REFERENCES users(username) ON DELETE CASCADE,
                        token TEXT UNIQUE NOT NULL,
                        expires_at TIMESTAMP NOT NULL,
                        revoked BOOLEAN NOT NULL DEFAULT FALSE,
                        created_at TIMESTAMP DEFAULT NOW()
                    );
                """)

                cur.execute("""
                    CREATE TABLE IF NOT EXISTS "organisationUnits" (
                        id TEXT PRIMARY KEY,
                        name TEXT,
                        shortname TEXT,
                        parent JSONB,
                        level BIGINT,
                        synced_at TIMESTAMP DEFAULT now()
                    );
                """)
                    
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS sync_state (
                        id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
                        last_sync TIMESTAMP WITH TIME ZONE
                    );
                    --INSERT INTO sync_state (last_sync) SELECT now() - INTERVAL '90 days' WHERE NOT EXISTS (SELECT 1 FROM sync_state);
                """)

            
            self.conn.commit()  # <- commit apr√®s cr√©ation
            self._verified_tables.add("base_tables")
        except Exception as e:
            self.conn.rollback()
            logger.exception(f"Erreur cr√©ation tables de base: {e}")
            raise
    
    # --------------------------
    # CREATION ADMIN PAR DEFAUT
    # --------------------------
    def create_default_admin(self):
        if "admin_created" in self._verified_tables and self.force_init is not True:
            return
        
        try:
            DFA = config.DEFAULT_ADMIN
            with self.conn.cursor(cursor_factory=RealDictCursor) as cur:
                # V√©rifier si un admin existe d√©j√†
                # cur.execute("SELECT * FROM users WHERE role='superadmin' LIMIT 1;")
                cur.execute("SELECT 1 FROM users WHERE username = %s LIMIT 1;", (DFA["username"],))
                if not cur.fetchone():
                    pw_hash = hash_password(DFA["password"])
                    cur.execute("""
                        INSERT INTO users (fullname, username, password, role)
                        VALUES (%s, %s, %s, %s)
                        RETURNING id, fullname, username, role;
                    """, (DFA["fullname"], DFA["username"], pw_hash, DFA["role"]))
                    self.conn.commit()
                    logger.info("‚úÖ Admin par d√©faut cr√©√© avec succ√®s")
            self._verified_tables.add("admin_created")
        except Exception as e:
            self.conn.rollback()
            logger.exception(f"Erreur cr√©ation admin: {e}")
            raise

    def ensure_table_exist_create_if_not(self, table: str, data: dict, id_field:str)->bool:
        """
        V√©rifie si la table existe, sinon la cr√©e automatiquement selon data.
        data est un dictionnaire avec un exemple de cl√©/valeur pour d√©tecter le type.
        """

        table = self.normalize_tablename(table)
        if table in self._verified_tables and self.force_init is not True:
            return True
        
        try:
            is_dict_data = True if data and isinstance(data, dict) else False

            if is_dict_data:
                object_id = data.get(id_field) if id_field else None
                if object_id is None:
                    raise ValueError(f"‚ùå Missing id_field '{id_field}'")
                
            with self.conn.cursor() as cur:
                # V√©rifie si la table existe
                # cur.execute("SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = %s);", (table,))
                cur.execute("SELECT to_regclass(%s);", (f'"{table}"',))  # IMPORTANT : regclass doit recevoir "NomExact"

                if cur.fetchone()[0]:
                    self._verified_tables.add(table)
                    return True

                if is_dict_data:
                    # Cr√©e la table si elle n'existe pas
                    columns = []
                    for col, val in data.items():
                        col_type = self.guess_pg_type(val)
                        # On ajoute 'PRIMARY KEY' pour l'ID si la cl√© est 'id_field'
                        primary_key_type = "PRIMARY KEY" if col == f'"{id_field}"' else ""
                        # # On ajoute 'NOT NULL' pour l'ID si la cl√© est 'id_field'
                        # not_null = "NOT NULL" if col == f'"{id_field}"' else ""
                        columns.append(f'"{col}" {col_type} {primary_key_type}'.strip())

                    create_query = f'CREATE TABLE "{table}" ({", ".join(columns)});'
                    cur.execute(create_query)
                    self.conn.commit()

                    logger.info(f"üÜï Table '{table}' cr√©√©e avec succ√®s.")
                    self._verified_tables.add(table)
            return False
        except Exception as e:
            self.conn.rollback()
            logger.exception(f"Erreur dans 'ensure_table_exist_create_if_not' : {e}")
            raise

    def check_if_exists(self, table:str, id_field:str, object_id:str):
        """
        V√©rifie si une entr√©e existe dans la base de donn√©es en fonction de l'ID.
        """
        try:
            table = self.normalize_tablename(table)

            if not id_field:
                raise ValueError(f"‚ùå Missing id_field '{id_field}'")
            
            if not object_id:
                raise ValueError(f"‚ùå Missing object_id '{object_id}'")
            
            with self.conn.cursor() as cursor:
                # cursor.execute(f"SELECT 1 FROM {table} WHERE {id_field} = %s LIMIT 1",(object_id,))
                query = sql.SQL("SELECT 1 FROM {} WHERE {} = %s").format(
                    sql.Identifier(table), sql.Identifier(id_field)
                )
                cursor.execute(query, (object_id,))
                return cursor.fetchone() is not None
        except Exception as e:
            logger.exception(f"Erreur lors de la v√©rification de l'existence : {e}")
            return False

    def ensure_columns_exist(self, table:str, data:dict, id_field:str):
        """
        V√©rifie que toutes les colonnes existent dans la table.
        Si une colonne n'existe pas -> elle est ajout√©e automatiquement.
        """
        table = self.normalize_tablename(table)
        if table not in self._verified_columns:
            self._verified_columns[table] = set()

        try:
            # Initialiser le cache si absent
            if data and isinstance(data, dict):
                missing_columns = [c for c in data if c not in self._verified_columns[table]]
                if not missing_columns:
                    return

                object_id = data.get(id_field) if id_field else None
                if object_id is None:
                    raise ValueError(f"‚ùå Missing id_field '{id_field}'")
        
                with self.conn.cursor() as cursor:
                    cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_name = %s", (table,))
                    existing_columns = {row[0] for row in cursor.fetchall()}
                    
                    for column, value in data.items():
                        if column not in existing_columns:
                            col_type = self.guess_pg_type(value)
                            cursor.execute(f'ALTER TABLE "{table}" ADD COLUMN "{column}" {col_type};')
                            logger.info(f"‚ûï Colonne ajout√©e: {column} ({col_type})")
                            self.conn.commit()
                        self._verified_columns[table].add(column)

        except Exception as e:
            self.conn.rollback()
            logger.exception("‚ùå Error with 'ensure_columns_exist' for %s: %s", object_id, e)


    def ensure_pk_or_unique(self, table, id_field):
        """
        V√©rifie que la colonne id_field est PRIMARY KEY ou UNIQUE.
        La cr√©e automatiquement si manquante.
        """
        key = (table, id_field)
        if key in self._verified_pk and self.force_init is not True:
            return

        query_check = """
            SELECT constraint_type
            FROM information_schema.table_constraints tc
            JOIN information_schema.constraint_column_usage ccu ON tc.constraint_name = ccu.constraint_name
            WHERE tc.table_name = %s AND ccu.column_name = %s AND tc.constraint_type IN ('PRIMARY KEY', 'UNIQUE');
        """

        with self.conn.cursor() as cur:
            cur.execute(query_check, (table, id_field))
            if not cur.fetchone(): # PK/unique non existant
                constraint = f"{table}_{id_field}_unique"
                alter = sql.SQL('ALTER TABLE {table} ADD CONSTRAINT {constraint} UNIQUE ({col});').format(
                    table=sql.Identifier(table),
                    constraint=sql.Identifier(constraint),
                    col=sql.Identifier(id_field)
                )
                cur.execute(alter)
                self.conn.commit()
                logger.info(f"‚úî Contrainte UNIQUE cr√©√©e sur {table}.{id_field}")

        self._verified_pk.add(key)


    def _insert_or_update(self, table:str, data:dict, id_field:str):
        """
        Insert or update PostgreSQL record.
        Automatically adds missing columns before upsert.
        """
        is_dict_data = True if data and isinstance(data, dict) else False

        if is_dict_data:
            object_id = data.get(id_field) if id_field else None
            if object_id is None:
                raise ValueError(f"  {table} -> ‚ùå Missing id_field '{id_field}' in payload")
        else:
            raise ValueError(f"  {table} -> ‚ùå data must be a dict")
     
        table = self.normalize_tablename(table)

        retries = 0
        while retries < config.MAX_RETRIES:
            try:
                # 1Ô∏è‚É£ Cr√©er la table si n√©cessaire avec les noms EXACTS
                self.ensure_table_exist_create_if_not(table, data, id_field)

                # üî• Auto-create columns if missing
                # 2Ô∏è‚É£ Ajouter colonnes manquantes sans modifier la casse
                self.ensure_columns_exist(table, data, id_field)

                # 3Ô∏è‚É£ Construire la requ√™te en respectant exactement les key names
                columns = list(data.keys())
                # values = list(data.values())
                values = [self.convert_value_for_pg(data[c]) for c in columns]   # ‚úî Correction cl√©

                # 2Ô∏è‚É£ UPDATE ou INSERT
                exists = self.check_if_exists(table, id_field, object_id)

                if exists:
                    # ‚Äî‚Äî‚Äî UPDATE ‚Äî‚Äî‚Äî
                    set_clause = ", ".join([f'"{c}" = %s' for c in columns])
                    query = f'UPDATE "{table}" SET {set_clause} WHERE "{id_field}" = %s'
                    params = values + [object_id]
                    op = f"‚úî UPDATE"

                else:
                    # ‚Äî‚Äî‚Äî INSERT ‚Äî‚Äî‚Äî
                    column_list = ", ".join([f'"{c}"' for c in columns])
                    placeholders = ", ".join(["%s"] * len(values))
                    query = f'INSERT INTO "{table}" ({column_list}) VALUES ({placeholders})'
                    params = values
                    op = f"‚úÖ INSERT"


                with self.conn.cursor() as cursor:
                    cursor.execute(query, params)
                    self.conn.commit()

                # print(f"{op} ‚úî {object_id}")
                logger.info(f"  {table} -> {op} r√©ussi pour {object_id}")
                return True

            except Exception as e:
                self.conn.rollback()   # üî• IMPORTANT : √©viter de bloquer la connexion
                retries += 1
                logger.exception(f"  {table} -> ‚ùå Insert/update failed for %s: %s", object_id, e)

                if retries < config.MAX_RETRIES:
                    logger.warning(f"‚è≥ Tentative {retries}/{config.MAX_RETRIES} √©chou√©e. Nouvelle tentative dans {config.RETRY_DELAY}s...")
                    sleep(config.RETRY_DELAY)
                else:
                    logger.error(f"  {table} -> ‚õî √âchec d√©finitif pour {object_id} apr√®s {config.MAX_RETRIES} tentatives")
                    return False

    def _bulk_insert_or_update(self, table: str, data: list, id_field: str):
        """
        Bulk UPSERT optimis√© (insert or update).
        - auto-cr√©ation table
        - auto-cr√©ation colonnes manquantes
        - transactions par batch
        - retry intelligent
        - millions de lignes support√©s
        """
        try:
            # üîç 0. Validation entr√©e
            if not isinstance(data, list):
                logger.error(f"  {table} -> ‚ùå data must be a list of dict")
                return False

            if len(data) == 0:
                logger.warning(f"Aucune donn√©e √† ins√©rer pour {table}")
                return False

            if not isinstance(data[0], dict):
                logger.error(f"  {table} -> ‚ùå Chaque √©l√©ment de data doit √™tre un dict")
                return False

            table = self.normalize_tablename(table)

            # ‚úÖ 1. Pr√©parer structure
            sample = data[0]

            if id_field not in sample:
                logger.error(f"  {table} -> ‚ùå Missing id_field '{id_field}' in payload records")
                return False

            # üîß Cr√©ation auto table + colonnes
            self.ensure_table_exist_create_if_not(table, sample, id_field)
            self.ensure_columns_exist(table, sample, id_field)
            self.ensure_pk_or_unique(table, id_field)

            columns = list(sample.keys())
            pg_columns = ', '.join(f'"{c}"' for c in columns)

            # üî• Colonnes √† update (toutes sauf id_field)
            update_columns = [c for c in columns if c != id_field]
            update_clause = ', '.join([f'"{c}" = EXCLUDED."{c}"' for c in update_columns])

            # Requ√™te UPSERT (INSERT ... ON CONFLICT)
            base_query = (f'INSERT INTO "{table}" ({pg_columns}) VALUES %s '
                        f'ON CONFLICT ("{id_field}") DO UPDATE SET {update_clause};')

            total_rows = len(data)
            logger.info(f"üöÄ BULK UPSERT de {total_rows} lignes ‚Üí {table}")

            cur = self.conn.cursor()


            # üöö 2. Process par batch
            for start in range(0, total_rows, config.BATCH_SIZE):

                batch = data[start:start + config.BATCH_SIZE]
                batch_tuples = [
                    tuple(self.convert_value_for_pg(row.get(c)) for c in columns)
                    for row in batch
                ]

                retries = 0
                batch_num = (start // config.BATCH_SIZE) + 1

                while retries <= config.MAX_RETRIES:
                    try:
                        execute_values(cur, base_query, batch_tuples)
                        self.conn.commit()
                        logger.info(f"‚úî Batch {batch_num} ({len(batch)} rows) upserted")
                        break

                    except (OperationalError, InterfaceError) as e:
                        self.conn.rollback()
                        retries += 1

                        if retries > config.MAX_RETRIES:
                            logger.error(f"  {table} -> ‚ùå √âCHEC FINAL batch {batch_num} apr√®s retries. Erreur: {e}")
                            return False

                        logger.warning(
                            f"‚ö† Erreur temporaire batch {batch_num}: {e}. "
                            f"Retry {retries}/{config.MAX_RETRIES} dans {config.RETRY_DELAY}s"
                        )
                        time.sleep(config.RETRY_DELAY)

                    except DatabaseError as e:
                        # Erreur SQL (colonne, type, table) ‚Üí non r√©cup√©rable
                        self.conn.rollback()
                        logger.error(f"  {table} -> ‚õî ERREUR SQL batch {batch_num}: {e}")
                        logger.error("üìå Type erreur : %s", type(e))
                        logger.error("üìå D√©tails : %s", e)

                        # Extract full PG error context if available
                        if hasattr(e, "pgerror"):
                            logger.error("üìå PG Error : %s", e.pgerror)

                        if hasattr(e, "diag"):
                            logger.error("üìå PG diag : %s", e.diag.message_primary)
                            logger.error("üìå Hint : %s", getattr(e.diag, "hint", None))
                            logger.error("üìå Detail : %s", getattr(e.diag, "detail", None))

                        logger.error("üìå Requ√™te SQL : %s", base_query)

                        # Dump the tuple that causes the crash
                        logger.error("üìå Exemple valeurs : %s", batch_tuples[:3])

                        return False

                    except Exception as e:
                        # Erreur inconnue ‚Üí non r√©cup√©rable
                        self.conn.rollback()
                        logger.error(f"‚ùå ERREUR inconnu batch {batch_num}: {e}")
                        return False

            cur.close()

            logger.info(f"  {table} -> üèÅ Bulk UPSERT termin√© : {total_rows} lignes ‚Üí {table}")
            return True

        except Exception as e:
            logger.error(f"  {table} -> ‚ùå ERREUR critique Bulk UPSERT: {e}")
            try:
                self.conn.rollback()
            except:
                pass
            return False


    # Stockage en base de donn√©e
    def upsert_data(self, table:str, data:dict) -> bool:
        id_field = DHIS2_TABLE_KEY[table]
        object_id = (data or {}).get(id_field) if id_field else None
        try:
            table = self.normalize_tablename(table)
            data['synced_at'] = datetime.now(timezone.utc)
            return self._insert_or_update(table, data, id_field)
        except Exception as e:
            logger.exception(f"  {table} -> ‚ùå Insert/update failed for %s: %s", object_id, e)
            return False
        
    # Stockage en bulk en base de donn√©e
    def bulk_upsert_data(self, table:str, dataList:list) -> bool:
        # üîç 0. Validation entr√©e
        if not isinstance(dataList, list):
            logger.error(f"  {table} -> ‚ùå data must be a list of dict")
            return False
        
        if len(dataList) > 0:
            id_field = DHIS2_TABLE_KEY[table]
            try:
                table = self.normalize_tablename(table)
                synced_at = datetime.now(timezone.utc)
                for data in dataList:
                    data['synced_at'] = synced_at
                return self._bulk_insert_or_update(table, dataList, id_field)
            except Exception as e:
                logger.exception(f"  {table} -> ‚ùå Insert/update failed: %s", e)
                return False
        
    # Suppression de donn√©e
    def delete_data(self, table: str, record_id: str) -> bool:
        """
        Supprime un enregistrement dans `table` bas√© sur `id_field = record_id`.

        Retourne :
            True  ‚Üí si un enregistrement a √©t√© supprim√©
            False ‚Üí si rien n'a √©t√© supprim√© ou en cas d'erreur
        """

        id_field = DHIS2_TABLE_KEY[table]

        if not table or not record_id or not id_field:
            raise ValueError(f"‚ùå Arguments invalides : table: {table}, record_id: {record_id} et id_field: {id_field} sont obligatoires.")

        try:
            table = self.normalize_tablename(table)

            with self.conn.cursor() as cursor:
                delete_query = sql.SQL('DELETE FROM "{}" WHERE {} = %s').format(sql.Identifier(table), sql.Identifier(id_field))

                cursor.execute(delete_query, (record_id,))
                deleted_rows = cursor.rowcount  # Nombre de lignes supprim√©es

            # Commit de la transaction
            self.conn.commit()

            return deleted_rows > 0

        except Exception as e:
            print(f"‚ùå Erreur lors de la suppression dans '{table}': {e}")
            try:
                self.conn.rollback()
            except:
                pass
            return False

    # Suppression  en bulk de donn√©e
    def bulk_delete_data(self, table: str, record_ids: list) -> bool:
        """
        Suppression massive (bulk delete) d'enregistrements DHIS2.
        - Haute performance
        - Gestion des erreurs + retries
        - Suppression par batch
        - Auto-d√©tection du champ cl√© via DHIS2_TABLE_KEY[table]

        Args:
            table (str): Nom de la table PostgreSQL
            record_ids (list): Liste d'IDs √† supprimer (valeurs simples)

        Returns:
            bool: True si suppression r√©ussie, False en cas d'√©chec.
        """

        # --- üîç Validation ---
        if not table or table not in DHIS2_TABLE_KEY:
            raise ValueError(f"‚ùå Table inconnue ou invalide : '{table}'")

        if not isinstance(record_ids, list) or len(record_ids) == 0:
            logger.warning(f"Aucun ID fourni pour suppression dans {table}")
            return False

        id_field = DHIS2_TABLE_KEY[table]

        table = self.normalize_tablename(table)

        logger.info(f"üóëÔ∏è Bulk delete ‚Üí {table}: {len(record_ids)} IDs")

        batch_size = getattr(config, "BATCH_SIZE", 5000)

        try:
            cur = self.conn.cursor()

            # --- üöÄ Suppression par batch ---
            for start in range(0, len(record_ids), batch_size):
                batch = record_ids[start:start + batch_size]

                retries = 0
                batch_num = start // batch_size + 1

                delete_query = sql.SQL('DELETE FROM "{}" WHERE {} = ANY(%s)').format(sql.Identifier(table), sql.Identifier(id_field))

                while retries <= config.MAX_RETRIES:
                    try:
                        cur.execute(delete_query, (batch,))
                        self.conn.commit()

                        logger.info(
                            f"‚úî Batch {batch_num} DELETE ({cur.rowcount} lignes supprim√©es)"
                        )
                        break

                    except (OperationalError, InterfaceError) as e:
                        self.conn.rollback()
                        retries += 1

                        if retries > config.MAX_RETRIES:
                            raise

                        logger.warning(
                            f"‚ö† Erreur r√©seau batch {batch_num}: {e}. "
                            f"Retry {retries}/{config.MAX_RETRIES} dans {config.RETRY_DELAY}s"
                        )
                        time.sleep(config.RETRY_DELAY)

                    except DatabaseError as e:
                        self.conn.rollback()
                        logger.error(f"‚õî Erreur SQL batch {batch_num}: {e}")
                        raise

                    except Exception as e:
                        self.conn.rollback()
                        logger.error(f"‚ùå Erreur inconnue batch {batch_num}: {e}")
                        raise

        finally:
            try:
                cur.close()
            except:
                pass

        logger.info(f"üèÅ Bulk DELETE termin√© ‚Üí {table}: {len(record_ids)} IDs supprim√©s")
        return True

    # R√©cup√©ration en base de donn√©e
    def _list_data(self, table: str,fields: list | tuple | None = None,*,limit: int | None = None,offset: int | None = None,filters: dict | None = None,order_by: str = "synced_at",order_dir: str = "DESC") -> list[dict[str, Any]]:
        """
        R√©cup√®re une liste g√©n√©rique depuis n'importe quelle table PostgreSQL.

        Params:
            table       : nom de la table
            fields      : liste des colonnes √† s√©lectionner (default: *)
            limit       : nombre max de lignes
            offset      : d√©calage (pagination)
            filters     : dict {col: value} pour WHERE
            order_by    : colonne de tri
            order_dir   : ASC ou DESC
        """

        # --- Validation s√©curis√©e ---
        if not isinstance(table, str):
            raise ValueError("table must be a string")
        
        table = self.normalize_tablename(table)

        if fields and not isinstance(fields, (list, tuple)):
            raise ValueError("fields must be list or tuple")

        if order_dir.upper() not in ("ASC", "DESC"):
            raise ValueError("order_dir must be 'ASC' or 'DESC'")

        # --- Colonnes √† s√©lectionner (SELECT)---
        if fields:
            fields_sql = ", ".join([f'"{c}"' for c in fields])
        else:
            fields_sql = "*"

        # --- Construction dynamique du WHERE ---
        where_clauses = []
        values = []

        if filters:
            for col, val in filters.items():
                where_clauses.append(f'"{col}" = %s')
                values.append(val)

        where_sql = ""
        if where_clauses:
            where_sql = "WHERE " + " AND ".join(where_clauses)

        # --- ORDER BY ---
        order_sql = f'ORDER BY "{order_by}" {order_dir}'

        # --- LIMIT / OFFSET ---
        limit_sql = ""
        if limit and limit > 0:
            limit_sql += " LIMIT %s"
            values.append(limit)

        if offset and offset > 0:
            limit_sql += " OFFSET %s"
            values.append(offset)

        # --- SQL finale ---
        query = f'SELECT {fields_sql} FROM "{table}" {where_sql} {order_sql} {limit_sql}'

        # --- Execution ---
        with self.conn.cursor() as cur:
            cur.execute(query, tuple(values))
            colnames = [desc[0] for desc in cur.description]
            rows = cur.fetchall()

        # --- conversion: tuple ‚Üí dict ---
        result = []
        for row in rows:
            row_dict = {}
            for col, val in zip(colnames, row):
                # Convertir datetime ‚Üí ISO
                if hasattr(val, "isoformat"):
                    row_dict[col] = val.isoformat()
                else:
                    row_dict[col] = val
            result.append(row_dict)

        return result

    def list_orgunits(self, level:int=None, only_ids:bool=False) -> list[dict[str, Any]]:
        # id_field = 'id'
        isGoodLevel = level is not None and isinstance(level,int) and level > 0
        filters={"level": level} if isGoodLevel else None
        
        orgunits = self._list_data("organisationUnits", filters=filters)

        if only_ids is True:
            orgunits_ids = [ou["id"] for ou in orgunits]
            return orgunits_ids
        
        return orgunits
        
    def list_dataelement(self) -> list[dict[str, Any]]:
        # id_field = 'id'
        # fields=["id","name","code","shortName", "created", "synced_at"]
        fields=["id","name","shortName", "synced_at"]
        return self._list_data("dataElements", fields=fields)

    def list_tei(self) -> list[dict[str, Any]]:
        # id_field = 'trackedEntityInstance'
        return self._list_data("trackedEntityInstances")
    
    def list_enrollment(self) -> list[dict[str, Any]]:
        # id_field = 'enrollment'
        return self._list_data("enrollments")

    def list_event(self) -> list[dict[str, Any]]:
        # id_field = 'event'
        return self._list_data("events")
    
    def list_attributes(self) -> list[dict[str, Any]]:
        # id_field = 'event'
        return self._list_data("attributes")
    
    def get_last_sync(self):
        default_date = to_datetime("2022-01-01T00:00:00")
        try:
            with self.conn.cursor() as cur:
                cur.execute("SELECT last_sync FROM sync_state ORDER BY id DESC LIMIT 1;")
                row = cur.fetchone()
                # Aucun enregistrement
                if not row or not row[0]:
                    return default_date
                value = row[0]
                # Si la valeur est d√©j√† un datetime PostgreSQL ‚Üí OK
                if isinstance(value, datetime):
                    return value
                # Sinon convertir (si string)
                return to_datetime(value)
        except Exception:
            return default_date


    def update_last_sync(self, new_dt: datetime):
        try:
            with self.conn.cursor() as cur:
                # Always ensure new_dt is a valid datetime object  
                if not isinstance(new_dt, datetime):
                    raise ValueError("new_dt must be a datetime object")
                # Check if a row already exists
                cur.execute("SELECT id FROM sync_state ORDER BY id DESC LIMIT 1;")
                row = cur.fetchone()
                if row:
                    # Update last_sync in the existing row
                    cur.execute("UPDATE sync_state SET last_sync = %s WHERE id = %s;",(new_dt, row[0]))
                else:
                    # Insert a new sync_state row
                    cur.execute("INSERT INTO sync_state (last_sync) VALUES (%s);",(new_dt,))
            self.conn.commit()
            return True
        except Exception as e:
            self.conn.rollback()   # IMPORTANT: ensures DB is not left in a bad state
            return False
